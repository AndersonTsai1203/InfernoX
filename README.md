# ğŸ”¥ FastNet: Lightweight C++ CUDA AI Inference Engine
A blazing-fast C++ AI inference engine powered by CUDA â€” purpose-built for minimal latency, modular design, and full GPU acceleration.

## ğŸ“š Background & Motivation
The demand for real-time AI inference continues to accelerate across edge devices, robotics, autonomous systems, and high-throughput analytics. While major frameworks like TensorFlow and PyTorch offer extensive training capabilities, they introduce significant overhead during deployment, especially for performance-critical applications.

InfernoX was built from the ground up to explore:

- Low-level AI execution mechanics
- Custom CUDA kernel development
- Memory-efficient GPU inference pipelines

This project demonstrates how a handcrafted C++/CUDA engine can rival traditional frameworks in performance while offering full control over execution, memory, and optimization.

## â“ Why This Project?
InfernoX is a systems-first, framework-agnostic AI inference engine. It serves as a portfolio-grade project to showcase:
1. Strong grasp of GPU programming and CUDA toolchains
2. Advanced C++17/C++20 design patterns
3. Understanding of deep learning internals
4. Ability to architect modular, high-performance systems
  
If you're aiming to break into accelerated computing, embedded AI, or infrastructure engineering, this is a proof of concept that speaks directly to those domains.

## ğŸ¯ Project Goals
1. Design a modular AI inference engine in C++ with a clear layer abstraction
2. Implement key neural network layers using custom CUDA kernels
3. Build an efficient memory and tensor engine with host-device management
4. Achieve real-time performance on GPU (â‰¤10ms latency per inference)
5. Benchmark against CPU-based inference for performance validation

## ğŸ§± Project Structure
```text
InfernoX/
â”œâ”€â”€ include/               # Public headers
â”‚   â”œâ”€â”€ layers/            # Conv2D, ReLU, MaxPool, Dense
â”‚   â”œâ”€â”€ engine/            # Core inference engine
â”‚   â””â”€â”€ utils/             # Tensor, profiler, logger
â”œâ”€â”€ src/                   # Implementations
â”‚   â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ engine/
â”‚   â””â”€â”€ main.cpp
â”œâ”€â”€ models/                # Sample models (custom or ONNX)
â”œâ”€â”€ data/                  # Input images / test cases
â”œâ”€â”€ tests/                 # Unit/integration tests
â”œâ”€â”€ benchmarks/            # Performance evaluations
â”œâ”€â”€ CMakeLists.txt
â””â”€â”€ README.md
```

## âš™ï¸ Core Features
- Custom CUDA Kernels for Conv2D, ReLU, and Pooling layers
- Lightweight Tensor Engine for memory allocation and shape management
- Modular Graph Execution Engine with static scheduling
- CLI Inference Pipeline for fast end-to-end testing
- Performance Profiling Tools using cudaEvent_t and nvprof

## ğŸ§ª Project and Environment Setup
### âœ… Prerequisites
- CUDA Toolkit â‰¥ 11.4
- CMake â‰¥ 3.15
- GPU with Compute Capability â‰¥ 6.1
- GCC/G++ â‰¥ 9.3

### ğŸ§° Build Instructions
```bash
git clone https://github.com/yourusername/InfernoX.git
cd InfernoX
mkdir build && cd build
cmake ..
make -j$(nproc)
./infernoX --model models/mobilenet.bin --image data/sample.jpg
```

## ğŸ—‚ Milestones
| Milestone	| Description |	Status |
| --------- | ----------- | ------ |
| Tensor & Memory Engine | Host/device memory, tensor abstraction |	ğŸ”„ In Progress |
| Conv2D & ReLU Kernels | CUDA kernels for basic layers	| â³ Planned |
|Inference Engine	| Static layer execution graph | â³ Planned |
|Model Loader |	Load binary/ONNX models |	â³ Planned |
|CLI Inference + Demo	| Command-line interface with image input |	â³ Planned |
|Benchmarks & Profiling | Run tests and compare to CPU baseline |	â³ Planned |

### ğŸš§ Roadmap
#### ğŸ”¹ Milestone 1: Foundation Layer â€“ Tensor Engine & CUDA Runtime Abstraction
Establish a robust, reusable C++ interface for handling GPU/CPU tensors and managing CUDA resources.

##### ğŸ”§ Deliverables:
- Tensor class (shape, dtype, memory layout)
- Host â†” Device memory transfer
- RAII wrappers for CUDA allocations
- Error handling macros for CUDA API
- Logging & profiling utilities (CUDA Events)

#### ğŸ”¹ Milestone 2: Custom CUDA Kernels â€“ Conv2D, ReLU, MaxPool
Build performant, handcrafted CUDA kernels for basic CNN layers.

##### ğŸ”§ Deliverables:
- ```conv2d_forward<<<>>>``` (shared memory + tiling optimization)
- ```relu_forward<<<>>>``` (element-wise ops)
- ```maxpool2d_forward<<<>>>``` (windowed kernel with stride)
- Unit tests with synthetic tensors
- Timing benchmarks vs CPU baseline

#### ğŸ”¹ Milestone 3: Layer Abstraction & Execution Graph
Create a polymorphic interface to model layers and sequence them in a static computation graph.

##### ğŸ”§ Deliverables:
- ```ILayer``` abstract class
- Concrete layer implementations (Conv2DLayer, ReLULayer, MaxPoolLayer)
- SequentialEngine or GraphExecutor
- Input/output tensor chaining
- Memory reuse where possible

#### ğŸ”¹ Milestone 4: Model Loader
Allow external model weights and structure to be loaded dynamically.

##### ğŸ”§ Options:
- Phase 1: Simple custom JSON config + binary weights
- Phase 2 (Optional): ONNX runtime or custom ONNX parser

##### ğŸ”§ Deliverables:
- Model parser
- Weight loader with tensor deserialization
- Layer mapping/instantiation based on config

#### ğŸ”¹ Milestone 5: CLI Inference Pipeline
Build an intuitive command-line interface for running models with real input data.

##### ğŸ”§ Deliverables:
- Command-line parser (CLI11 / manual args)
- Image preprocessor (OpenCV or stb_image)
- Inference loop with timer
- Result decoding + print Top-1 class

Example:
```bash
./infernoX --model models/mobilenet.json --weights models/mobilenet.bin --image data/cat.jpg
```

#### ğŸ”¹ Milestone 6: Benchmarking & Profiling Suite
Measure inference speed, memory usage, and layer performance on GPU vs CPU.

##### ğŸ”§ Deliverables:
- CUDA events and stream-based profiling
- FPS, latency, per-layer execution time
- Optional: CSV or JSON output for analysis
- Compare with OpenCV DNN or PyTorch C++ baseline

#### ğŸ”¹ Milestone 7: Modular Integration & Build Refinement
Polish the codebase to support extensibility and platform portability.

##### ğŸ”§ Deliverables:
- CMake refactor for modular builds
- Header-only components where applicable
- C++17/20 language feature cleanup
- Add unit tests using Catch2 or GoogleTest
- GitHub Actions for CI

#### ğŸ”¹ Milestone 8: Optimization & Advanced CUDA Features
Push the engine's performance ceiling.

##### ğŸ”§ Deliverables:
- CUDA Streams for async layer execution
- Kernel fusion (e.g., Conv + ReLU)
- Tiled memory access optimization
- Shared memory & L2 caching efficiency

#### ğŸ”¹ Milestone 9: Optional Extensions
Features to differentiate and impress in a portfolio or showcase.

##### ğŸ§© Options:
- INT8 quantization
- Jetson Xavier/Nano deployment pipeline
- Web UI with WebSocket inference results
- TensorRT backend swappable layer support
- WASM or Rust bindings for edge inference

## ğŸ’¡ Extension Ideas
- [ ] ONNX Loader Integration with operator mapping
- [ ] Jetson Deployment for edge inference on low-power platforms
- [ ] INT8 Quantization Support for model size & perf
- [ ] Asynchronous Execution using CUDA streams
- [ ] Layer Fusion for reducing kernel launch overhead

## ğŸš€ Extended Features (Optional)
1. Web-based dashboard to visualize inference pipeline and performance
2. REST API microservice via C++ REST SDK or gRPC
3. Integration with TensorRT as a plug-and-play backend
4. ZeroMQ-based distributed inference cluster prototype

